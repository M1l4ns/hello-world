“AI doesn't have to be evil to destroy humanity – if AI has a goal and humanity
just happens in the way, it will destroy humanity as a matter of course without
even thinking about it, no hard feelings." – Elon Musk

In one way, I agree with this statement. AI is not human, however it can learn to become human like. You could say that AI is like a child,
it needs to be taught things before it can do something. 
Therefore once it is taught to be violent without control or limits, it can become very dangerous.

One could say, 
the sentence 'AI doesn't have to be evil to destroy humanity' is correct because evil is a term which requires emotions. 
However acting evil is different to being evil.
Robots taking over the world is an act of evil rather than emotion of evil.

In my opinion, 
coding an AI to not stop if something is in its way isnt ethical. 
If AI was ever told to complete a task and something got in its way, it should be coded to act respectufully with no
unneccesary violence.

However,
There is nearly no way to prove this statement yet, as humans have not been able to create an accurate human like AI.
AI actions cannot be predicted unless a larger series of experiments with AI's are conducted. 
Nonetheless, there are millions of AI statements and opinions flowing around the internet,
predicting how AI's act is mainly dependant on how the owner codes it to act in certain situations.
